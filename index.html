<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Representational Alignment under Flexible Evaluation: When the Best-Fitting Model Isn’t the Right One</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link href="https://fonts.googleapis.com/css2?family=Merriweather&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
  <!-- Font Awesome CDN (תקין) -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" />
</head>
<body>
  <nav class="navbar">
    <a href="#abstract">Abstract</a>
    <a href="#introduction">Introduction</a>
    <a href="#figures">Figures</a>
  </nav>

  <div class="container">
    <h1>Representational Alignment under Flexible Evaluation: When the Best-Fitting Model Isn’t the Right One</h1>
    <p class="anonymous-note"><em>Anonymous Author(s)</em></p>
    <p>
      <a id="code" href="#">
        <i class="fab fa-github github-icon"></i>
        View Code and Data
      </a>
    </p>

    <hr class="section-divider" />

    <h2 id="abstract">Abstract</h2>
    <p class="section-text">
      Linearly transforming stimulus representations of deep neural networks yields
2 high-performing models of behavioral and neural response to complex stimuli.
3 But does the test accuracy of such predictions identify genuine representational
4 alignment? We addressed this question through a large-scale model-recovery study.
5 Twenty diverse vision models were linearly aligned to 4.3 million behavioral judg-
6 ments from the THINGS-odd-one-out dataset and calibrated to reproduce human
7 response entropy. For each model in turn, we sampled synthetic responses from its
8 probabilistic predictions, fitted all candidate models to the synthetic data, and tested
9 whether the data-generating model would re-emerge as the best predictor of the sim-
10 ulated data. Model recovery accuracy improved with training-set size but plateaued
11 below 80%, even at 5.25 million simulated trials. Regression analyses linked
12 misidentification primarily to shifts in representational geometry induced by the
13 linear transformation, as well as to the effective dimensionality of the transformed
14 features. These findings demonstrate that, even with massive data, overly flexible
15 alignment metrics may fail to guide us toward artificial representations that are
16 genuinely more human-aligned. Model comparison experiments must be designed
17 to balance the trade-off between predictive accuracy and specificity—ensuring the
18 best-fitting model is also the right one.
    </p>

    <hr class="section-divider" />
    
    <h2 id="introduction">Introduction</h2>
    <p class="section-text">
      20 The search for mechanistic explanations of human cognition, in combination with rapid advances
21 in deep learning, has motivated the use of stimulus representations in pretrained neural networks as
22 models of the biological representation of complex stimuli. Even without modification, stimulus
23 representations in neural networks trained on visual tasks show surprising correspondence with
24 cortical visual representations [1–3] and visual perceptual judgments [4–6]. When evaluation is made
25 flexible by fitting linear weights to improve the alignment of neural network representations with
26 brain [7–9] or behavioral data [10, 11], this approach achieves predictive accuracy that exceeds that of
27 any other computational model. A major assumption underlying current research on representational
28 alignment between humans and deep neural networks (DNNs) is that a neural network whose
29 representations are more predictive of brain or behavioral data is a better model of biological
30 representations and computations. For models evaluated without further data-driven fitting, high
31 predictive accuracy occurring by chance is unlikely. However, once flexible, data-driven fitting
32 procedures are employed, an important question arises: does predictive accuracy under flexible
33 evaluation truly reflect representational alignment? [12–14]. This question carries weight since there
34 are good reasons to employ flexible evaluation. A complete yet fully emergent representational
35 alignment is unlikely even for models whose processing is qualitatively similar to that of humans.
36 Furthermore, inter-individual variability may also motivate flexible alignment metrics [15]. And yet,
37 flexible evaluation may incur a hidden cost: when each model is allowed to adjust to best fit brain or Submitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Do not distribute.
    </p>

    <hr class="section-divider" />

    <h2 id="figures">Figures</h2>
    <div class="controls">
      <label for="figureSelect">Choose a figure:</label>
      <select id="figureSelect">
        <option value="fig1">Figure 1</option>
        <option value="fig2">Figure 2</option>
        <option value="fig3">Figure 3</option>
        <option value="fig4">Figure 4</option>
        <option value="fig5">Figure 5</option>
      </select>
    </div>

    <div class="figure-container">
      <iframe
        id="figureDisplay"
        src="figures/meta_plot_swapped_axes_top_acc.pdf"
        width="100%"
        height="500px"
        style="border: 1px solid #ccc; border-radius: 8px;"
      ></iframe>
      <h3 id="figureFileName">meta_plot_swapped_axes_top_acc.pdf</h3>
    </div>

    <div class="figure-descriptions">
      <p class="figure-text" id="fig1-text">
        <strong>Figure 1:</strong><br>
        <strong>(A)</strong> Model recovery accuracy across different dataset sizes.<br>
        <strong>(B, C, D)</strong> Confusion matrices at three training set sizes.<br>
        Each matrix row corresponds to the data-generating model, and each matrix column to the recovered model.<br>
        Diagonal entries represent correct model recovery.
      </p>
      <p class="figure-text" id="fig2-text">
        <strong>Figure 2:</strong><br>
        Model test accuracy on the THINGS odd-one-out dataset over gradient levels of linear-probing flexibility.<br>
        <strong>(A)</strong> Zero-shot evaluation.<br>
        <strong>(B)</strong> Diagonal transformation matrix.<br>
        <strong>(C)</strong> Rectangular transformation (p×10).<br>
        <strong>(D)</strong> Square transformation (p×p).
      </p>
      <p class="figure-text" id="fig3-text">
        <strong>Figure 3:</strong><br>
        <strong>(A)</strong> Mean rank of the data-generating model ordered by predictive accuracy.<br>
        <strong>(B)</strong> Mean rank of each model when it is <em>not</em> the data-generating model.
      </p>
      <p class="figure-text" id="fig4-text">
        <strong>Figure 4:</strong><br>
        Model recovery accuracy as a function of “best-case” predictive accuracy.<br>
        Each point reflects a probe flexibility and training set size.
      </p>
      <p class="figure-text" id="fig5-text">
        <strong>Figure 5:</strong><br>
        Change in representational dissimilarity before and after applying a linear transformation.<br>
        Visualized using MDS: red dot = original embedding; green arrow = probing shift.<br>
        VICE model used as the most human-like representational space.
      </p>
    </div>

    <hr class="section-divider" />
    <footer class="footer">
      <p>This project was created with ❤️ for anonymous academic review.</p>
    </footer>
  </div>

  <script src="script.js"></script>
</body>
</html>
